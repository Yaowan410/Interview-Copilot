# Eval Cases (MVP)

## Coding (expected doc_type=coding)

1) Question: Two Sum — explain approach and give Python code.
   Expected: doc_id=two_sum sections: Key Idea, Python Template, Complexity, Pitfalls

2) Question: Valid Parentheses — why stack? provide code + edge cases.
   Expected: doc_id=valid_parentheses sections: Key Idea, Python Template, Edge Cases

3) Question: Binary Search — write iterative code and explain loop condition.
   Expected: doc_id=binary_search sections: Step-by-Step Approach, Python Template, Common Mistakes

4) Question: Merge Two Sorted Lists — explain dummy node and complexity.
   Expected: doc_id=merge_two_sorted_lists sections: Key Idea, Python Template, Complexity

5) Question: When would binary search fail even if code is correct?
   Expected: doc_id=binary_search sections: Edge Cases, Common Mistakes

6) Question: For valid parentheses, what if input includes other characters?
   Expected: doc_id=valid_parentheses sections: Follow-Up, Key Idea

## Concept (expected doc_type=concept)

7) Question: What is self-attention? Explain Q/K/V and why we scale by sqrt(d_k).
   Expected: doc_id=self_attention sections: Mechanism, Why Scaling by √d_k

8) Question: Why do we need positional encoding in Transformers?
   Expected: doc_id=positional_encoding sections: Intuition, Mechanism, Common Misconceptions

9) Question: What is multi-head attention and why is it better than single-head?
   Expected: doc_id=multi_head_attention sections: Intuition, Why Multiple Heads, Dimensionality

10) Question: What is a Transformer encoder layer made of?
    Expected: doc_id=transformer_encoder sections: Definition, High-Level Architecture

11) Question: Compare learned positional embeddings vs sinusoidal.
    Expected: doc_id=positional_encoding sections: Learned Positional Encoding, Limitations

12) Question: What are the limitations of self-attention for long context?
    Expected: doc_id=self_attention sections: Complexity, Limitations